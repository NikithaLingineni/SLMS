{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**DistilBERT**"
      ],
      "metadata": {
        "id": "HkOiVVSSwiqN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5rVV_i3twYY3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"Datasets/final_parkinsons_dataset.csv\")\n",
        "print(df[\"Label\"].value_counts(normalize=True))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download(\"stopwords\")\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"[^a-z\\s']\", \"\", text)\n",
        "    text = \" \".join([word for word in text.split() if word not in stop_words])\n",
        "    return text\n",
        "\n",
        "df[\"Cleaned_Abstract\"] = df[\"Abstract\"].astype(str).apply(clean_text)\n",
        "print(df[[\"Abstract\", \"Cleaned_Abstract\"]].head())"
      ],
      "metadata": {
        "id": "Rk6l1VvXwsMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DistilBertTokenizer\n",
        "\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "tokens = tokenizer(\n",
        "    df[\"Cleaned_Abstract\"].tolist(),\n",
        "    padding=True, truncation=True, max_length=512,\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "\n",
        "print(tokens[\"input_ids\"].shape)\n",
        "print(\"Attention Mask shape:\", tokens[\"attention_mask\"].shape)"
      ],
      "metadata": {
        "id": "mm2cqglbxBgD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "\n",
        "labels = torch.tensor(df[\"Label\"].values)\n",
        "\n",
        "train_inputs, test_inputs, train_labels, test_labels, train_masks, test_masks = train_test_split(\n",
        "    tokens[\"input_ids\"], labels, tokens[\"attention_mask\"],\n",
        "    test_size=0.2, random_state=42, stratify=labels\n",
        ")\n",
        "print(f\"Train Inputs: {train_inputs.shape}, Train Masks: {train_masks.shape}, Train Labels: {train_labels.shape}\")\n",
        "print(f\"Test Inputs: {test_inputs.shape}, Test Masks: {test_masks.shape}, Test Labels: {test_labels.shape}\")"
      ],
      "metadata": {
        "id": "j1dc4R8ExD3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import DistilBertForSequenceClassification\n",
        "from torch.optim import AdamW\n",
        "\n",
        "device = torch.device(\"cuda:6\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "model = DistilBertForSequenceClassification.from_pretrained(\n",
        "    \"distilbert-base-uncased\",\n",
        "    num_labels=2,\n",
        "    torch_dtype=torch.float32\n",
        ")\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=1e-5, eps=1e-8)\n",
        "\n",
        "print(f\"Model Loaded Successfully on {device}!\")"
      ],
      "metadata": {
        "id": "MpH3MpUPxI2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "class ParkinsonsDataset(Dataset):\n",
        "    def __init__(self, inputs, masks, labels):\n",
        "        self.inputs = inputs\n",
        "        self.masks = masks\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            \"input_ids\": self.inputs[idx],\n",
        "            \"attention_mask\": self.masks[idx],\n",
        "            \"labels\": self.labels[idx]\n",
        "        }\n"
      ],
      "metadata": {
        "id": "xuRfYCtexa1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = ParkinsonsDataset(train_inputs, train_masks, train_labels)\n",
        "test_dataset = ParkinsonsDataset(test_inputs, test_masks, test_labels)"
      ],
      "metadata": {
        "id": "6nJi4iwbxd-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
        "print(\"DataLoaders created successfully!\")"
      ],
      "metadata": {
        "id": "AW2fypMuxhI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "from transformers import get_scheduler\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "=num_training_steps = len(train_dataloader) * 5\n",
        "lr_scheduler = get_scheduler(\n",
        "    \"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
        ")\n",
        "\n",
        "model.train()\n",
        "\n",
        "epochs = 25\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for batch in train_dataloader:\n",
        "        batch_inputs = batch[\"input_ids\"].to(device)\n",
        "        batch_masks = batch[\"attention_mask\"].to(device)\n",
        "        batch_labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        if torch.isnan(batch_inputs).any() or torch.isnan(batch_masks).any() or torch.isnan(batch_labels).any():\n",
        "            print(\"NaN detected in batch input! Skipping...\")\n",
        "            continue\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(input_ids=batch_inputs, attention_mask=batch_masks)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        logits = torch.clamp(logits, min=-1e6, max=1e6)\n",
        "\n",
        "        loss = loss_fn(logits, batch_labels)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        correct += (preds == batch_labels).sum().item()\n",
        "        total += batch_labels.size(0)\n",
        "\n",
        "    avg_loss = total_loss / len(train_dataloader)\n",
        "    accuracy = correct / total\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}] - Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
        "print(\"Training Completed!\")"
      ],
      "metadata": {
        "id": "vPHXzgtGxkFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "total_loss = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_dataloader:\n",
        "        batch_inputs = batch[\"input_ids\"].to(device)\n",
        "        batch_masks = batch[\"attention_mask\"].to(device)\n",
        "        batch_labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        outputs = model(input_ids=batch_inputs, attention_mask=batch_masks)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        loss = loss_fn(logits, batch_labels)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        correct += (preds == batch_labels).sum().item()\n",
        "        total += batch_labels.size(0)\n",
        "\n",
        "avg_test_loss = total_loss / len(test_dataloader)\n",
        "test_accuracy = correct / total\n",
        "\n",
        "print(f\"Test Loss: {avg_test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "Xo9LghaXxq_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"distilbert_parkinsons_model\")\n",
        "tokenizer.save_pretrained(\"distilbert_parkinsons_model\")\n"
      ],
      "metadata": {
        "id": "dSG9lxKuxttn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer\n",
        "\n",
        "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert_parkinsons_model\")\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert_parkinsons_model\")\n"
      ],
      "metadata": {
        "id": "ABLf3hoxxwyA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "device = torch.device(\"cuda:6\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_dataloader:\n",
        "        batch_inputs = batch[\"input_ids\"].to(device)\n",
        "        batch_masks = batch[\"attention_mask\"].to(device)\n",
        "        batch_labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        outputs = model(input_ids=batch_inputs, attention_mask=batch_masks)\n",
        "        preds = torch.argmax(outputs.logits, dim=1)\n",
        "\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(batch_labels.cpu().numpy())\n",
        "\n",
        "print(classification_report(all_labels, all_preds, digits=4))\n"
      ],
      "metadata": {
        "id": "PDjPsbY1x1vV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "plt.figure(figsize=(5,5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[\"Non PD\", \"PD\"], yticklabels=[\"Non PD\", \"PD\"])\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ovmO_y7ux4Bd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "probs = []\n",
        "with torch.no_grad():\n",
        "    for batch in test_dataloader:\n",
        "        batch_inputs = batch[\"input_ids\"].to(device)\n",
        "        batch_masks = batch[\"attention_mask\"].to(device)\n",
        "\n",
        "        outputs = model(input_ids=batch_inputs, attention_mask=batch_masks)\n",
        "        probs.extend(torch.nn.functional.softmax(outputs.logits, dim=1)[:, 1].cpu().numpy())\n",
        "\n",
        "\n",
        "fpr, tpr, _ = roc_curve(all_labels, probs)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.plot(fpr, tpr, color=\"blue\", label=f\"AUC = {roc_auc:.4f}\")\n",
        "plt.plot([0, 1], [0, 1], color=\"gray\", linestyle=\"--\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"AUC-ROC Curve\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "qj-6Qua8x6J2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BioBERT**"
      ],
      "metadata": {
        "id": "gtORJozVyhaf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"Datasets/final_parkinsons_dataset.csv\")\n",
        "\n",
        "print(df[\"Label\"].value_counts(normalize=True))\n"
      ],
      "metadata": {
        "id": "zswQxeU9yj3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download(\"stopwords\")\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"[^a-z\\s']\", \"\", text)\n",
        "    text = \" \".join([word for word in text.split() if word not in stop_words])\n",
        "    return text\n",
        "\n",
        "df[\"Cleaned_Abstract\"] = df[\"Abstract\"].astype(str).apply(clean_text)\n",
        "\n",
        "print(df[[\"Abstract\", \"Cleaned_Abstract\"]].head())\n"
      ],
      "metadata": {
        "id": "rMa9mTSlywZ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-base-cased-v1.1\")\n",
        "\n",
        "tokens = tokenizer(\n",
        "    df[\"Cleaned_Abstract\"].tolist(),\n",
        "    padding=True, truncation=True, max_length=512,\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "\n",
        "print(tokens[\"input_ids\"].shape)\n",
        "print(\"Attention Mask shape:\", tokens[\"attention_mask\"].shape)\n"
      ],
      "metadata": {
        "id": "WfuNXE9wyyCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "\n",
        "labels = torch.tensor(df[\"Label\"].values)\n",
        "\n",
        "train_inputs, test_inputs, train_labels, test_labels, train_masks, test_masks = train_test_split(\n",
        "    tokens[\"input_ids\"], labels, tokens[\"attention_mask\"],\n",
        "    test_size=0.2, random_state=42, stratify=labels\n",
        ")\n",
        "print(f\"Train Inputs: {train_inputs.shape}, Train Masks: {train_masks.shape}, Train Labels: {train_labels.shape}\")\n",
        "print(f\"Test Inputs: {test_inputs.shape}, Test Masks: {test_masks.shape}, Test Labels: {test_labels.shape}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "H8FG6ipeyzQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "from torch.optim import AdamW\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"dmis-lab/biobert-base-cased-v1.1\", num_labels=2)\n",
        "\n",
        "device = torch.device(\"cuda:6\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
        "\n",
        "print(\"BioBERT Model Loaded Successfully!\")\n"
      ],
      "metadata": {
        "id": "D08DoW9ay0uA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "class ParkinsonsDataset(Dataset):\n",
        "    def __init__(self, inputs, masks, labels):\n",
        "        self.inputs = inputs\n",
        "        self.masks = masks\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            \"input_ids\": self.inputs[idx],\n",
        "            \"attention_mask\": self.masks[idx],\n",
        "            \"labels\": self.labels[idx]\n",
        "        }\n"
      ],
      "metadata": {
        "id": "ko_3-AbGy2cw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = ParkinsonsDataset(train_inputs, train_masks, train_labels)\n",
        "test_dataset = ParkinsonsDataset(test_inputs, test_masks, test_labels)"
      ],
      "metadata": {
        "id": "y-mWX6jhy-jP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
        "print(\"DataLoaders created successfully!\")"
      ],
      "metadata": {
        "id": "yZyGbAI0zAPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "from transformers import get_scheduler\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "num_training_steps = len(train_dataloader) * 5\n",
        "lr_scheduler = get_scheduler(\n",
        "    \"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
        ")\n",
        "\n",
        "model.train()\n",
        "\n",
        "epochs = 25\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for batch in train_dataloader:\n",
        "        batch_inputs = batch[\"input_ids\"].to(device)\n",
        "        batch_masks = batch[\"attention_mask\"].to(device)\n",
        "        batch_labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(input_ids=batch_inputs, attention_mask=batch_masks)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        loss = loss_fn(logits, batch_labels)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        correct += (preds == batch_labels).sum().item()\n",
        "        total += batch_labels.size(0)\n",
        "\n",
        "    avg_loss = total_loss / len(train_dataloader)\n",
        "    accuracy = correct / total\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}] - Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "print(\"Training Completed!\")\n"
      ],
      "metadata": {
        "id": "kCbojo9uzBcv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "total_loss = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_dataloader:\n",
        "        batch_inputs = batch[\"input_ids\"].to(device)\n",
        "        batch_masks = batch[\"attention_mask\"].to(device)\n",
        "        batch_labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        outputs = model(input_ids=batch_inputs, attention_mask=batch_masks)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        loss = loss_fn(logits, batch_labels)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        correct += (preds == batch_labels).sum().item()\n",
        "        total += batch_labels.size(0)\n",
        "\n",
        "avg_test_loss = total_loss / len(test_dataloader)\n",
        "test_accuracy = correct / total\n",
        "\n",
        "print(f\"Test Loss: {avg_test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "BNnlFRjozCmq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"biobert_parkinsons_model\")\n",
        "tokenizer.save_pretrained(\"biobert_parkinsons_model\")\n"
      ],
      "metadata": {
        "id": "fY7abs7OzD3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"dmis-lab/biobert-base-cased-v1.1\", num_labels=2)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-base-cased-v1.1\")\n",
        "\n",
        "print(\"BioBERT Loaded Successfully!\")\n"
      ],
      "metadata": {
        "id": "F7sZDfAQzNZz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "device = torch.device(\"cuda:6\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_dataloader:\n",
        "        batch_inputs = batch[\"input_ids\"].to(device)\n",
        "        batch_masks = batch[\"attention_mask\"].to(device)\n",
        "        batch_labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        outputs = model(input_ids=batch_inputs, attention_mask=batch_masks)\n",
        "        preds = torch.argmax(outputs.logits, dim=1)\n",
        "\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(batch_labels.cpu().numpy())\n",
        "\n",
        "print(classification_report(all_labels, all_preds, digits=4))\n"
      ],
      "metadata": {
        "id": "JWOOp1AazPP8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "plt.figure(figsize=(5,5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[\"Non PD\", \"PD\"], yticklabels=[\"Non PD\", \"PD\"])\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "XCpcelcN1qk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "probs = []\n",
        "with torch.no_grad():\n",
        "    for batch in test_dataloader:\n",
        "        batch_inputs = batch[\"input_ids\"].to(device)\n",
        "        batch_masks = batch[\"attention_mask\"].to(device)\n",
        "\n",
        "        outputs = model(input_ids=batch_inputs, attention_mask=batch_masks)\n",
        "        probs.extend(torch.nn.functional.softmax(outputs.logits, dim=1)[:, 1].cpu().numpy())\n",
        "\n",
        "fpr, tpr, _ = roc_curve(all_labels, probs)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.plot(fpr, tpr, color=\"blue\", label=f\"AUC = {roc_auc:.4f}\")\n",
        "plt.plot([0, 1], [0, 1], color=\"gray\", linestyle=\"--\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"AUC-ROC Curve\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "DWUpNtdP1tVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ALBERT**"
      ],
      "metadata": {
        "id": "-PuXOe7V2WCk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"Datasets/final_parkinsons_dataset.csv\")\n",
        "\n",
        "print(df[\"Label\"].value_counts(normalize=True))\n"
      ],
      "metadata": {
        "id": "xw00ATVd2j-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download(\"stopwords\")\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"[^a-z\\s']\", \"\", text)\n",
        "    text = \" \".join([word for word in text.split() if word not in stop_words])\n",
        "    return text\n",
        "\n",
        "df[\"Cleaned_Abstract\"] = df[\"Abstract\"].astype(str).apply(clean_text)\n",
        "\n",
        "print(df[[\"Abstract\", \"Cleaned_Abstract\"]].head())\n"
      ],
      "metadata": {
        "id": "FfJVuDaZ2s4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"albert-base-v2\")\n",
        "\n",
        "tokens = tokenizer(\n",
        "    df[\"Cleaned_Abstract\"].tolist(),\n",
        "    padding=True, truncation=True, max_length=512,\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "\n",
        "print(tokens[\"input_ids\"].shape)\n",
        "print(\"Attention Mask shape:\", tokens[\"attention_mask\"].shape)\n"
      ],
      "metadata": {
        "id": "oKN-H_CE2uVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "\n",
        "labels = torch.tensor(df[\"Label\"].values)\n",
        "\n",
        "train_inputs, test_inputs, train_labels, test_labels, train_masks, test_masks = train_test_split(\n",
        "    tokens[\"input_ids\"], labels, tokens[\"attention_mask\"],\n",
        "    test_size=0.2, random_state=42, stratify=labels\n",
        ")\n",
        "print(f\"Train Inputs: {train_inputs.shape}, Train Masks: {train_masks.shape}, Train Labels: {train_labels.shape}\")\n",
        "print(f\"Test Inputs: {test_inputs.shape}, Test Masks: {test_masks.shape}, Test Labels: {test_labels.shape}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "OFOkLoQY2vga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "from torch.optim import AdamW\n",
        "\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"albert-base-v2\", num_labels=2)\n",
        "\n",
        "device = torch.device(\"cuda:7\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
        "\n",
        "print(\"ALBERT Model Loaded Successfully!\")\n"
      ],
      "metadata": {
        "id": "dHc9OlTx2ws0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "class ParkinsonsDataset(Dataset):\n",
        "    def __init__(self, inputs, masks, labels):\n",
        "        self.inputs = inputs\n",
        "        self.masks = masks\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            \"input_ids\": self.inputs[idx],\n",
        "            \"attention_mask\": self.masks[idx],\n",
        "            \"labels\": self.labels[idx]\n",
        "        }\n"
      ],
      "metadata": {
        "id": "Tn_37M5G2yr4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = ParkinsonsDataset(train_inputs, train_masks, train_labels)\n",
        "test_dataset = ParkinsonsDataset(test_inputs, test_masks, test_labels)"
      ],
      "metadata": {
        "id": "sSvMLyQQ26Ue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
        "print(\"DataLoaders created successfully!\")"
      ],
      "metadata": {
        "id": "cGMAHNa_27dh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "from transformers import get_scheduler\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "num_training_steps = len(train_dataloader) * 5\n",
        "lr_scheduler = get_scheduler(\n",
        "    \"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
        ")\n",
        "\n",
        "model.train()\n",
        "\n",
        "epochs = 25\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for batch in train_dataloader:\n",
        "        batch_inputs = batch[\"input_ids\"].to(device)\n",
        "        batch_masks = batch[\"attention_mask\"].to(device)\n",
        "        batch_labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(input_ids=batch_inputs, attention_mask=batch_masks)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        loss = loss_fn(logits, batch_labels)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        correct += (preds == batch_labels).sum().item()\n",
        "        total += batch_labels.size(0)\n",
        "\n",
        "    avg_loss = total_loss / len(train_dataloader)\n",
        "    accuracy = correct / total\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}] - Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "print(\"Training Completed!\")\n"
      ],
      "metadata": {
        "id": "ys5wWBcE286a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "total_loss = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_dataloader:\n",
        "        batch_inputs = batch[\"input_ids\"].to(device)\n",
        "        batch_masks = batch[\"attention_mask\"].to(device)\n",
        "        batch_labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        outputs = model(input_ids=batch_inputs, attention_mask=batch_masks)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        loss = loss_fn(logits, batch_labels)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        correct += (preds == batch_labels).sum().item()\n",
        "        total += batch_labels.size(0)\n",
        "\n",
        "avg_test_loss = total_loss / len(test_dataloader)\n",
        "test_accuracy = correct / total\n",
        "\n",
        "print(f\"Test Loss: {avg_test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "M4x545aa2_Nl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"albert_parkinsons_model\")\n",
        "tokenizer.save_pretrained(\"albert_parkinsons_model\")\n"
      ],
      "metadata": {
        "id": "G7rqtpsP3Ac5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"albert_parkinsons_model\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"albert_parkinsons_model\")\n",
        "\n",
        "print(\"ALBERT Model and Tokenizer Loaded Successfully!\")\n"
      ],
      "metadata": {
        "id": "IsUKyyoO3KI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "device = torch.device(\"cuda:7\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_dataloader:\n",
        "        batch_inputs = batch[\"input_ids\"].to(device)\n",
        "        batch_masks = batch[\"attention_mask\"].to(device)\n",
        "        batch_labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        outputs = model(input_ids=batch_inputs, attention_mask=batch_masks)\n",
        "        preds = torch.argmax(outputs.logits, dim=1)\n",
        "\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(batch_labels.cpu().numpy())\n",
        "\n",
        "print(classification_report(all_labels, all_preds, digits=4))\n"
      ],
      "metadata": {
        "id": "Hg_UZ8Z73M3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "plt.figure(figsize=(5,5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[\"Non PD\", \"PD\"], yticklabels=[\"Non PD\", \"PD\"])\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Go0ZuPYo3OI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "probs = []\n",
        "with torch.no_grad():\n",
        "    for batch in test_dataloader:\n",
        "        batch_inputs = batch[\"input_ids\"].to(device)\n",
        "        batch_masks = batch[\"attention_mask\"].to(device)\n",
        "\n",
        "        outputs = model(input_ids=batch_inputs, attention_mask=batch_masks)\n",
        "        probs.extend(torch.nn.functional.softmax(outputs.logits, dim=1)[:, 1].cpu().numpy())\n",
        "\n",
        "fpr, tpr, _ = roc_curve(all_labels, probs)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.plot(fpr, tpr, color=\"blue\", label=f\"AUC = {roc_auc:.4f}\")\n",
        "plt.plot([0, 1], [0, 1], color=\"gray\", linestyle=\"--\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"AUC-ROC Curve\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "lw1FNvI-3PSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TinyBERT**"
      ],
      "metadata": {
        "id": "GQbdvIwc4Mqf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"Datasets/final_parkinsons_dataset.csv\")\n",
        "\n",
        "print(df[\"Label\"].value_counts(normalize=True))\n"
      ],
      "metadata": {
        "id": "bdDdYyQK4O-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download(\"stopwords\")\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"[^a-z\\s']\", \"\", text)\n",
        "    text = \" \".join([word for word in text.split() if word not in stop_words])\n",
        "    return text\n",
        "\n",
        "df[\"Cleaned_Abstract\"] = df[\"Abstract\"].astype(str).apply(clean_text)\n",
        "\n",
        "print(df[[\"Abstract\", \"Cleaned_Abstract\"]].head())\n"
      ],
      "metadata": {
        "id": "4ksBu8qh4XuU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"huawei-noah/TinyBERT_General_6L_768D\")\n",
        "\n",
        "tokens = tokenizer(\n",
        "    df[\"Cleaned_Abstract\"].tolist(),\n",
        "    padding=True, truncation=True, max_length=512,\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "\n",
        "print(tokens[\"input_ids\"].shape)\n",
        "print(\"Attention Mask shape:\", tokens[\"attention_mask\"].shape)\n"
      ],
      "metadata": {
        "id": "KjwtEO5R4ZPP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "\n",
        "labels = torch.tensor(df[\"Label\"].values)\n",
        "\n",
        "train_inputs, test_inputs, train_labels, test_labels, train_masks, test_masks = train_test_split(\n",
        "    tokens[\"input_ids\"], labels, tokens[\"attention_mask\"],\n",
        "    test_size=0.2, random_state=42, stratify=labels\n",
        ")\n",
        "print(f\"Train Inputs: {train_inputs.shape}, Train Masks: {train_masks.shape}, Train Labels: {train_labels.shape}\")\n",
        "print(f\"Test Inputs: {test_inputs.shape}, Test Masks: {test_masks.shape}, Test Labels: {test_labels.shape}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "VpaNdWTf4aYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "from torch.optim import AdamW\n",
        "\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"huawei-noah/TinyBERT_General_6L_768D\", num_labels=2)\n",
        "\n",
        "device = torch.device(\"cuda:6\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
        "\n",
        "print(\"TinyBERT Model Loaded Successfully!\")\n"
      ],
      "metadata": {
        "id": "i1VlRMIT4b9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "class ParkinsonsDataset(Dataset):\n",
        "    def __init__(self, inputs, masks, labels):\n",
        "        self.inputs = inputs\n",
        "        self.masks = masks\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            \"input_ids\": self.inputs[idx],\n",
        "            \"attention_mask\": self.masks[idx],\n",
        "            \"labels\": self.labels[idx]\n",
        "        }\n"
      ],
      "metadata": {
        "id": "RGfR71gH4dU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = ParkinsonsDataset(train_inputs, train_masks, train_labels)\n",
        "test_dataset = ParkinsonsDataset(test_inputs, test_masks, test_labels)"
      ],
      "metadata": {
        "id": "4M1IaNFc4lYe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
        "print(\"DataLoaders created successfully!\")"
      ],
      "metadata": {
        "id": "DujrOAFF4m11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "from transformers import get_scheduler\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "num_training_steps = len(train_dataloader) * 5\n",
        "lr_scheduler = get_scheduler(\n",
        "    \"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
        ")\n",
        "\n",
        "model.train()\n",
        "\n",
        "epochs = 25\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for batch in train_dataloader:\n",
        "        batch_inputs = batch[\"input_ids\"].to(device)\n",
        "        batch_masks = batch[\"attention_mask\"].to(device)\n",
        "        batch_labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(input_ids=batch_inputs, attention_mask=batch_masks)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        loss = loss_fn(logits, batch_labels)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        correct += (preds == batch_labels).sum().item()\n",
        "        total += batch_labels.size(0)\n",
        "\n",
        "    avg_loss = total_loss / len(train_dataloader)\n",
        "    accuracy = correct / total\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}] - Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "print(\"Training Completed!\")\n"
      ],
      "metadata": {
        "id": "5xFGiQ0T4nyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "total_loss = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_dataloader:\n",
        "        batch_inputs = batch[\"input_ids\"].to(device)\n",
        "        batch_masks = batch[\"attention_mask\"].to(device)\n",
        "        batch_labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        outputs = model(input_ids=batch_inputs, attention_mask=batch_masks)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        loss = loss_fn(logits, batch_labels)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        correct += (preds == batch_labels).sum().item()\n",
        "        total += batch_labels.size(0)\n",
        "\n",
        "avg_test_loss = total_loss / len(test_dataloader)\n",
        "test_accuracy = correct / total\n",
        "\n",
        "print(f\"Test Loss: {avg_test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "lv6PVc4F4o9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"tinybert_parkinsons_model\")\n",
        "tokenizer.save_pretrained(\"tinybert_parkinsons_model\")\n"
      ],
      "metadata": {
        "id": "Jy9HWGZZ4qVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"huawei-noah/TinyBERT_General_6L_768D\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"huawei-noah/TinyBERT_General_6L_768D\")\n",
        "\n",
        "print(\"TinyBERT Model and Tokenizer Loaded Successfully!\")\n"
      ],
      "metadata": {
        "id": "laUkHDqg4zHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "device = torch.device(\"cuda:6\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_dataloader:\n",
        "        batch_inputs = batch[\"input_ids\"].to(device)\n",
        "        batch_masks = batch[\"attention_mask\"].to(device)\n",
        "        batch_labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        outputs = model(input_ids=batch_inputs, attention_mask=batch_masks)\n",
        "        preds = torch.argmax(outputs.logits, dim=1)\n",
        "\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(batch_labels.cpu().numpy())\n",
        "\n",
        "print(classification_report(all_labels, all_preds, digits=4))\n"
      ],
      "metadata": {
        "id": "HMYSNCoT40fB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "plt.figure(figsize=(5,5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[\"Non PD\", \"PD\"], yticklabels=[\"Non PD\", \"PD\"])\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "p_8g_Zi141sO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "probs = []\n",
        "with torch.no_grad():\n",
        "    for batch in test_dataloader:\n",
        "        batch_inputs = batch[\"input_ids\"].to(device)\n",
        "        batch_masks = batch[\"attention_mask\"].to(device)\n",
        "\n",
        "        outputs = model(input_ids=batch_inputs, attention_mask=batch_masks)\n",
        "        probs.extend(torch.nn.functional.softmax(outputs.logits, dim=1)[:, 1].cpu().numpy())\n",
        "\n",
        "fpr, tpr, _ = roc_curve(all_labels, probs)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.plot(fpr, tpr, color=\"blue\", label=f\"AUC = {roc_auc:.4f}\")\n",
        "plt.plot([0, 1], [0, 1], color=\"gray\", linestyle=\"--\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"AUC-ROC Curve\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "eMp7L0oB42tR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}